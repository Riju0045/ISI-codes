{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9Q4lstdnKurZ6yeUZuClk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riju0045/ISI-codes/blob/main/ISI_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6J-1VKL9DAW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('orders_export_1 (45)(in).csv')\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['Billing street'].fillna(df.groupby('Name')['Billing street'].transform('first'), inplace=True)\n",
        "df['Billing Address 1'].fillna(df.groupby('Name')['Billing Address 1'].transform('first'), inplace=True)\n",
        "df['Billing address 2'].fillna(df.groupby('Name')['Billing address 2'].transform('first'), inplace=True)\n",
        "df['Billing City'].fillna(df.groupby('Name')['Billing City'].transform('first'), inplace=True)\n",
        "df['Billing Zip'].fillna(df.groupby('Name')['Billing Zip'].transform('first'), inplace=True)\n",
        "df['Billing Province'].fillna(df.groupby('Name')['Billing Province'].transform('first'), inplace=True)\n",
        "df['Billing Country'].fillna(df.groupby('Name')['Billing Country'].transform('first'), inplace=True)\n",
        "df['Shipping name'].fillna(df.groupby('Name')['Shipping name'].transform('first'), inplace=True)\n",
        "df['Shipping Address'].fillna(df.groupby('Name')['Shipping Address'].transform('first'), inplace=True)\n",
        "df['Shipping Address 1'].fillna(df.groupby('Name')['Shipping Address 1'].transform('first'), inplace=True)\n",
        "df['Shipping City'].fillna(df.groupby('Name')['Shipping City'].transform('first'), inplace=True)\n",
        "#df['Shipping City'].fillna(df.groupby('Name')['Shipping '].transform('first'), inplace=True)\n",
        "df['Shipping Zip'].fillna(df.groupby('Name')['Shipping Zip'].transform('first'), inplace=True)\n",
        "df['Shipping Province'].fillna(df.groupby('Name')['Shipping Province'].transform('first'), inplace=True)\n",
        "df['Shipping Country'].fillna(df.groupby('Name')['Shipping Country'].transform('first'), inplace=True)\n",
        "\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "5e_BP5HROE-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "\n",
        "df.to_csv('cleaned_orders.csv', index=False)\n",
        "files.download('cleaned_orders.csv')"
      ],
      "metadata": {
        "id": "BcFTLEZ7S2YI",
        "outputId": "88c4f038-61f2-42c5-f7a1-6d774516a3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1c38c8b2-c509-43e8-886b-7aa98a4803ff\", \"cleaned_orders.csv\", 16226961)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "file_path = 'D.xlsx'\n",
        "df_unique = pd.read_excel(file_path, sheet_name='unique')\n",
        "if 'df_unique' in locals() and 'Shipping Method' in df_unique.columns:\n",
        "    shipping_counts = df_unique['Shipping Method'].value_counts()\n",
        "\n",
        "\n",
        "    colors = sns.color_palette('husl', len(shipping_counts))\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.pie(shipping_counts,\n",
        "            labels=None,\n",
        "            autopct='%1.1f%%',\n",
        "            startangle=140,\n",
        "            colors=colors,\n",
        "            wedgeprops={'edgecolor': 'white', 'linewidth': 1.5})\n",
        "\n",
        "    plt.title('Distribution of Shipping Methods', fontsize=16)\n",
        "    plt.axis('equal')\n",
        "\n",
        "\n",
        "    plt.legend(shipping_counts.index, title=\"Shipping Methods\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('shipping_method_distribution.jpg')\n",
        "    from google.colab import files\n",
        "    files.download('shipping_method_distribution.jpg')\n",
        "else:\n",
        "    print(\"Error: DataFrame 'df_unique' or 'Shipping method' column not found.\")\n",
        "    print(\"Please ensure the 'unique' sheet from 'D.xlsx' is loaded correctly and contains 'Shipping method'.\")\n"
      ],
      "metadata": {
        "id": "fxTgXCQGhrLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if 'df_new' in locals() and 'Month' in df_new.columns and 'Refunded Amount' in df_new.columns:\n",
        "\n",
        "\n",
        "    month_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
        "                   'July', 'August', 'September', 'October', 'November', 'December']\n",
        "    df_new['Month'] = pd.Categorical(df_new['Month'], categories=month_order, ordered=True)\n",
        "\n",
        "\n",
        "    df_new['Refunded Amount'] = pd.to_numeric(df_new['Refunded Amount'], errors='coerce')\n",
        "\n",
        "\n",
        "    df_new.dropna(subset=['Refunded Amount'], inplace=True)\n",
        "\n",
        "\n",
        "    monthly_refunds = df_new.groupby('Month')['Refunded Amount'].sum().reset_index()\n",
        "\n",
        "\n",
        "    monthly_refunds = monthly_refunds.sort_values(by='Month')\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.lineplot(x='Month', y='Refunded Amount', data=monthly_refunds, marker='o')\n",
        "\n",
        "    plt.title('Total Refunded Amount per Month', fontsize=16)\n",
        "    plt.xlabel('Month', fontsize=12)\n",
        "    plt.ylabel('Total Refunded Amount', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Error: DataFrame 'df_new' or necessary columns ('Month', 'Refunded Amount') not found.\")\n",
        "    print(\"Please ensure the 'new' sheet from 'D.xlsx' is loaded correctly and contains these columns.\")\n",
        "    print(\"Also, check that the column containing refunded amounts is named 'Refunded Amount'.\")"
      ],
      "metadata": {
        "id": "JkuU6yRI2aoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "file_path = 'D.xlsx'\n",
        "df_new = pd.read_excel(file_path, sheet_name='new')\n",
        "\n",
        "if 'df_new' in locals() and 'Day' in df_new.columns and 'Month' in df_new.columns and 'Hour' in df_new.columns:\n",
        "\n",
        "\n",
        "    df_new['Day_Identifier'] = df_new['Month'].astype(str) + '_' + df_new['Day'].astype(str)\n",
        "\n",
        "\n",
        "    num_unique_days = df_new['Day_Identifier'].nunique()\n",
        "\n",
        "    if num_unique_days > 0:\n",
        "\n",
        "        order_count_per_hour = df_new.groupby('Hour').size()\n",
        "\n",
        "\n",
        "        average_orders_per_hour = order_count_per_hour / num_unique_days\n",
        "\n",
        "\n",
        "        average_orders_per_hour = average_orders_per_hour.sort_index()\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        ax = sns.barplot(x=average_orders_per_hour.index, y=average_orders_per_hour.values, palette='viridis')\n",
        "\n",
        "        plt.title('Average Number of Orders per Hour', fontsize=16)\n",
        "        plt.xlabel('Hour', fontsize=12)\n",
        "        plt.ylabel('Average Number of Orders', fontsize=12)\n",
        "        plt.xticks(rotation=0)\n",
        "\n",
        "\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, fmt='%.2f', label_type='edge')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Error: No unique days found in the data to calculate average.\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: DataFrame 'df_new' or necessary columns ('Day', 'Month', 'Hour') not found.\")\n",
        "    print(\"Please ensure the 'new' sheet from 'D.xlsx' is loaded correctly and contains these columns.\")\n",
        "    print(\"Also, make sure the DataFrame is named 'df_new'.\")"
      ],
      "metadata": {
        "id": "tQtqrNI4mErS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.savefig('Average_Orders_per_Hour.png')\n",
        "files.download('Average_Orders_per_Hour.png')"
      ],
      "metadata": {
        "id": "yQUp43Zen9V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if 'df_original' not in locals() or df_original.empty:\n",
        "    file_path = 'orders_export_1 (45)(in).csv'\n",
        "    try:\n",
        "        df_original = pd.read_csv(file_path)\n",
        "        print(\"Loaded df_original from CSV.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        df_original = pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the CSV: {e}\")\n",
        "        df_original = pd.DataFrame()\n",
        "\n",
        "\n",
        "if not df_original.empty and 'Name' in df_original.columns and 'Lineitem name' in df_original.columns:\n",
        "\n",
        "\n",
        "    df_items = df_original[['Name', 'Lineitem name']].copy()\n",
        "\n",
        "    one_hot_pivot = df_items.pivot_table(index='Name', columns='Lineitem name', aggfunc='size', fill_value=0)\n",
        "\n",
        "    one_hot_binary = (one_hot_pivot > 0).astype(int)\n",
        "\n",
        "\n",
        "    one_hot_encoded_df = one_hot_binary.reset_index()\n",
        "\n",
        "\n",
        "    ordered_names = df_original['Name'].unique()\n",
        "\n",
        "\n",
        "    name_order_map = {name: i for i, name in enumerate(ordered_names)}\n",
        "\n",
        "    one_hot_encoded_df['order'] = one_hot_encoded_df['Name'].map(name_order_map)\n",
        "\n",
        "\n",
        "    one_hot_encoded_df = one_hot_encoded_df.sort_values(by='order')\n",
        "\n",
        "\n",
        "    one_hot_encoded_df = one_hot_encoded_df.drop(columns=['order'])\n",
        "\n",
        "\n",
        "    print(\"One-Hot Encoded Dataset:\")\n",
        "    print(one_hot_encoded_df.head())\n",
        "    print(f\"\\nShape of the one-hot encoded dataset: {one_hot_encoded_df.shape}\")\n",
        "    print(f\"Number of unique items: {len(one_hot_encoded_df.columns) - 1}\") # Exclude the 'Name' column\n",
        "\n",
        "\n",
        "    output_filename = 'one_hot_encoded_items_per_name.csv'\n",
        "    one_hot_encoded_df.to_csv(output_filename, index=False)\n",
        "    print(f\"\\nSaving and downloading '{output_filename}'...\")\n",
        "    files.download(output_filename)\n",
        "    print(f\"'{output_filename}' has been downloaded.\")\n",
        "\n",
        "else:\n",
        "    print(\"Could not proceed with one-hot encoding. Ensure 'df_original' is loaded and contains 'Name' and 'Lineitem name' columns.\")"
      ],
      "metadata": {
        "id": "GcVqxhutpcWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apyori\n",
        "\n",
        "from apyori import apriori\n",
        "\n",
        "if 'one_hot_encoded_df' in locals() and not one_hot_encoded_df.empty:\n",
        "\n",
        "    transactions = []\n",
        "\n",
        "    for index, row in one_hot_encoded_df.iterrows():\n",
        "\n",
        "        items_in_transaction = row[row == 1].index.tolist()\n",
        "\n",
        "        items_in_transaction = [item for item in items_in_transaction if item != 'Name']\n",
        "        if items_in_transaction:\n",
        "            transactions.append(items_in_transaction)\n",
        "\n",
        "    if not transactions:\n",
        "        print(\"No transactions found after processing the one-hot encoded data.\")\n",
        "    else:\n",
        "        print(f\"Prepared {len(transactions)} transactions for analysis.\")\n",
        "\n",
        "        rules = apriori(transactions,\n",
        "                        min_support=0.001,\n",
        "                        min_confidence=0.2,\n",
        "                        min_lift=3,\n",
        "                        min_length=2)\n",
        "\n",
        "        results = list(rules)\n",
        "\n",
        "        print(\"\\nFound Association Rules:\")\n",
        "        if not results:\n",
        "            print(\"No rules found with the given parameters. Try adjusting min_support, min_confidence, or min_lift.\")\n",
        "        else:\n",
        "\n",
        "            for item in results:\n",
        "\n",
        "                pair = item[0]\n",
        "                items = [x for x in pair]\n",
        "\n",
        "\n",
        "                support = item[1]\n",
        "\n",
        "\n",
        "                rules_list = item[2]\n",
        "\n",
        "                for rule in rules_list:\n",
        "\n",
        "                    antecedent = list(rule[0])\n",
        "\n",
        "                    consequent = list(rule[1])\n",
        "\n",
        "                    confidence = rule[2]\n",
        "\n",
        "                    lift = rule[3]\n",
        "\n",
        "                    print(f\"Rule: {antecedent} -> {consequent}\")\n",
        "                    print(f\"  Support: {support:.4f}\")\n",
        "                    print(f\"  Confidence: {confidence:.4f}\")\n",
        "                    print(f\"  Lift: {lift:.4f}\")\n",
        "                    print(\"-\" * 30)\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'one_hot_encoded_df' DataFrame not found or is empty.\")\n",
        "    print(\"Please ensure the preceding code to create 'one_hot_encoded_df' ran successfully.\")"
      ],
      "metadata": {
        "id": "NSW0OMzmtttx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rule_data = []\n",
        "if 'results' in locals() and results:\n",
        "    for item in results:\n",
        "        pair = item[0]\n",
        "        support = item[1]\n",
        "        rules_list = item[2]\n",
        "\n",
        "        for rule in rules_list:\n",
        "            antecedent = list(rule[0])\n",
        "            consequent = list(rule[1])\n",
        "            confidence = rule[2]\n",
        "            lift = rule[3]\n",
        "\n",
        "            rule_data.append({\n",
        "                'Antecedent': ', '.join(antecedent),\n",
        "                'Consequent': ', '.join(consequent),\n",
        "                'Support': support,\n",
        "                'Confidence': confidence,\n",
        "                'Lift': lift\n",
        "            })\n",
        "\n",
        "\n",
        "    rules_df = pd.DataFrame(rule_data)\n",
        "\n",
        "\n",
        "    output_filename = 'association_rules.csv'\n",
        "    rules_df.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"\\nAssociation rules saved to '{output_filename}'.\")\n",
        "\n",
        "    files.download(output_filename)\n",
        "    print(f\"'{output_filename}' has been downloaded.\")\n",
        "\n",
        "elif 'results' in locals() and not results:\n",
        "    print(\"No rules were found by the Apriori algorithm, so nothing to save.\")\n",
        "else:\n",
        "    print(\"Apriori results ('results' variable) not found. Please run the preceding cell to generate the rules.\")"
      ],
      "metadata": {
        "id": "yT0i1Y_s8FM7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}